# Reverse-engineering the connectome from iEEG to reveal synaptic homeostasis in sleep

Master thesis by Jan Segessenmann (April 2020 to September 2020)

23.04.2020



## 1. Introduction

#### 1.1 Question

Can we confirm the synaptic homeostasis hypothesis (|ğ‘Šğ‘ ğ‘™ğ‘’ğ‘’ğ‘|<|ğ‘Šğ‘¤ğ‘ğ‘˜ğ‘’|) [1] and how are the different sleep stages involved in the downscaling during sleep?

#### 1.2 Methods

Reproduce iEEG traces during wakefulness and sleep with RNNs to optimize connectivity matrix. Analyze behavior of weights to conclude on synaptic homeostasis hypothesis.

Extension: Include human connectome data from TVB [2].



## 2. RNN on iEEG data

A first approach aims at getting an intuition on how good a simple RNN can predict some iEEG channels based on others. A simple RNN was created and resulting connectivity matrix was analyzed.

Data from: http://ieeg-swez.ethz.ch/ (epilepsy research).

#### 2.1 RNN architecture

Simple RNN architecture with one layer containing input-, output- and hidden nodes. Some elements $\mathbf{i}$ of the hidden notes act as outputs. MSE is used as loss function.



$$
\begin{align*}
\mathbf{h}_t &= \tanh(\mathbf{b} + \mathbf{Wh}_{t-1} + \mathbf{Ux}_t) \\
\mathbf{\hat{y}}_t &= \mathbf{h}_t[\mathbf{i}] \\
L_t &= \textrm{MSE}(\mathbf{\hat{y}}_t, \mathbf{y}_t)
\end{align*}
$$



#### 2.2 Important Parameters

| Name                         | Quantity                   |
| :--------------------------- | -------------------------- |
| Sample size                  | 2000                       |
| Channel size                 | 66                         |
| Output channels              | Positions [10, 30, 50]     |
| Window size for one RNN step | 50 Samples                 |
| Hidden units                 | 10, $\mathbf{i}=[3, 5, 7]$ |



#### 2.3 Results and Discussion

Predictions of output channels of test data after seven epochs:

#### ![fig_0422_predictions_rnn](/home/jan/Documents/MThCode/doc/figures/fig_0422_predictions_rnn.png)

The hidden nodes $\mathbf{i} = [3, 5, 7]$ act as output for the channels 10, 30 and 50 respectively.

![fig_0422_weights_ih](/home/jan/Documents/MThCode/doc/figures/fig_0422_weights_ih.png)

![fig_0422_weights_hh](/home/jan/Documents/MThCode/doc/figures/fig_0422_weights_hh.png)

#### 2.4 Findings

* Predicted channels are mainly mapped from nearby channels.
* Positions of electrodes unknown, so interpretation is speculative.





## 3. FFNN/RNN on toy data

A second approach aims at creating a complete connectivity matrix on a downscaled problem with toy data to see eventual problems and solutions when working with real iEEG data.

Four channels of toy data were generated:

![fig_0422_toy_data](/home/jan/Documents/MThCode/doc/figures/fig_0422_toy_data.png)

#### 3.1 FFNN architecure

The NN architecture used, can either be expressed as RNN or FFNN on the unfolded graph. Both was done and similar results in predicting trajectories were achieved. However, the following focuses on FFNN architecture. Essentially, the architecture is similar to what we saw before and can be visualized as follows:

![fig_0422_nn_1-rotation](/home/jan/Documents/MThCode/doc/figures/fig_0422_nn_1-rotation.png)

There are three input nodes (grey) , one output node (white) and one optional hidden node (dashed lines, not used here). The window size is 50 samples  (only 4 samples are shown in the above visualization).

#### 3.2 Rotation

After training the learned weights are stored in the corresponding place of a connectivity matrix that assumes a FC layer. The inputs/output rotate afterwards, which could - for a second step - be visualized as follows:

![fig_0422_nn_2-rotation](/home/jan/Documents/MThCode/doc/figures/fig_0422_nn_2-rotation.png)

New weights are trained and stored. This rotation continues (four rotations in total) until all weights are learned for a FC layer:

![fig_0422_nn_full-rotation](/home/jan/Documents/MThCode/doc/figures/fig_0422_nn_full-rotation.png)

Now any three trajectories can predict the missing one with the learned weights.



#### 3.3 Results

The prediction of the first rotation is shown below. Channels 0 to 2 were inputs, channel 3 was output.

![fig_0422_result_1-rotation](/home/jan/Documents/MThCode/doc/figures/fig_0422_result_1-rotation.png)

After the full rotation, the following weights are stored:

![fig_0422_weights_rotation](/home/jan/Documents/MThCode/doc/figures/fig_0422_weights_rotation.png)

#### 3.4 Questions and Discussion

* It does what was expected (e.g. looking at the weights from the first rotation).

![fig_0422_toy_data_short](/home/jan/Documents/MThCode/doc/figures/fig_0422_toy_data_short.png)

* How many channels should act as input/output?

* Does this approach make sense in a biologically plausible way? In what sense can weights be interpreted as synapses?

  ![fig_0422_nn_bio](/home/jan/Documents/MThCode/doc/figures/fig_0422_nn_bio.png)

  

* Role of bias, normalization?

  



## References

[1] Tononi, Giulio, and Chiara Cirelli. â€œSleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration.â€ Neuron 81, no. 1 (January 8, 2014): 12â€“34. https://doi.org/10.1016/j.neuron.2013.12.025.

[2] Sanz Leon, Paula, Stuart A. Knock, M. Marmaduke Woodman, Lia Domide, Jochen Mersmann, Anthony R. McIntosh, and Viktor Jirsa. â€œThe Virtual Brain: A Simulator of Primate Brain Network Dynamics.â€ Frontiers in Neuroinformatics 7 (2013). https://doi.org/10.3389/fninf.2013.00010.



## Appendix

![fig_ieeg_data](/home/jan/Documents/MThCode/doc/figures/fig_0422_ieeg_data.png)

![fig_ieeg_data](/home/jan/Documents/MThCode/doc/figures/fig_0422_ieeg_correlation.png)

```python
class FFNN(nn.Module):
    def __init__(self, in_size, out_size):
        super().__init__()
        # Parameters
        self.in_size = in_size
        self.out_size = out_size
        self.channel_size = self.in_size + self.out_size
        # Create FC Layer
        fc_in = self.in_size + self.out_size
        fc_out = self.out_size
        self.fc = nn.Linear(fc_in, fc_out)
        # Initialize Weights
        k = np.sqrt(1 / fc_in)
        self.weights = torch.FloatTensor(self.channel_size, self.channel_size).uniform_(-											k, k)
        #self.weights = torch.zeros(self.channel_size, self.channel_size)
        self.bias = torch.zeros(self.channel_size)

    def save_weights(self, in_pos: list, out_pos: int):
        with torch.no_grad():
            for idx, pos in enumerate(in_pos):
                self.weights[pos, out_pos] = self.fc.weight[0][idx]
            self.weights[out_pos, out_pos] = self.fc.weight[0][-1]
            self.bias[out_pos] = self.fc.bias[0]

    def get_weights(self, in_pos: list, out_pos: int):
        with torch.no_grad():
            for idx, pos in enumerate(in_pos):
                self.fc.weight[0][idx] = self.weights[pos, out_pos]
            self.fc.weight[0][-1] = self.weights[out_pos, out_pos]
            self.fc.bias[0] = self.bias[out_pos]

    def forward(self, X):
        # Initialize hidden node
        h = torch.zeros(self.out_size, dtype=torch.float32)
        # Forward path
        for i in range(X.shape[0]):
            fc_in = torch.cat((X[i, :], h), 0)
            h = torch.tanh(self.fc(fc_in))
        return h
```


