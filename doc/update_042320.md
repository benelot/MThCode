# Reverse-engineering the connectome from iEEG to reveal synaptic homeostasis in sleep

Master thesis by Jan Segessenmann (March 2020 to September 2020)

Presentation date: 23.04.2020



## 1. Introduction

#### 1.1 Question

Can we confirm the synaptic homeostasis hypothesis (|ğ‘Šğ‘ ğ‘™ğ‘’ğ‘’ğ‘|<|ğ‘Šğ‘¤ğ‘ğ‘˜ğ‘’|) [1] and how are the different sleep stages involved in the downscaling during sleep?

#### 1.2 Methods

Reproduce iEEG traces during wakefulness and sleep with RNNs to optimize connectivity matrix. Analyze behavior of weights to conclude on synaptic homeostasis hypothesis.

Extension: Include human connectome data from TVB [2].



## 2. RNN on iEEG data

Data from: http://ieeg-swez.ethz.ch/ (epilepsy research)

#### 2.1 RNN architecture

Simple RNN architecture with one layer containing input-, output- and hidden nodes.  A gate $\mathbf{g}$ selects the hidden notes that act as outputs. MSE is used as loss function.



<img src="/home/jan/Documents/MThCode/doc/figures/fig_simple_rnn_architecture.png" alt="fig_simple_rnn_architecture" style="zoom:60%;" />
$$
\begin{align*}
\mathbf{h}_t &= \tanh(\mathbf{b} + \mathbf{Wh}_{t-1} + \mathbf{Ux}_t) \\
\mathbf{\hat{y}}_t &= \mathbf{g} \odot \mathbf{h}_t \\
L_t &= \textrm{MSE}(\mathbf{\hat{y}}_t, \mathbf{y}_t)
\end{align*}
$$



#### 2.2 Important Parameters

| Name                                   | Quantity     |
| :------------------------------------- | ------------ |
| Sample size                            | 2000         |
| Channel size                           | 66           |
| Output channels                        | [10, 30, 50] |
| Window size for one RNN step [samples] | 50           |



#### 2.3 Results







## 3. FFNN/RNN on toy

 data



```python
class FFNN(nn.Module):
    def __init__(self, input_size=len(ch_in), hidden_size=20):
        super().__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size
        self.output_size = output_size
            
        self.fc1 = nn.Linear(28 * 28, 200)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.log_softmax(x)
```



## References

[1] Tononi, Giulio, and Chiara Cirelli. â€œSleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration.â€ Neuron 81, no. 1 (January 8, 2014): 12â€“34. https://doi.org/10.1016/j.neuron.2013.12.025.

[2] Sanz Leon, Paula, Stuart A. Knock, M. Marmaduke Woodman, Lia Domide, Jochen Mersmann, Anthony R. McIntosh, and Viktor Jirsa. â€œThe Virtual Brain: A Simulator of Primate Brain Network Dynamics.â€ Frontiers in Neuroinformatics 7 (2013). https://doi.org/10.3389/fninf.2013.00010.



## Appendix

![fig_ieeg_data](/home/jan/Documents/MThCode/doc/figures/fig_ieeg_data.png)

![fig_ieeg_data](/home/jan/Documents/MThCode/doc/figures/fig_ieeg_correlation.png)

